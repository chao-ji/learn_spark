{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"pyspark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we normally pass functions to Spark, such as a **map()** function or a condition for **filter()**, they can use variables defined outside them in the driver program, but each task running on the cluster gets a new copy of each variable, and updates from these copies are not propagated back to the driver. Spark's shared variables, accumulators and broadcast variables, relax this restriction for two common types of communication patterns: \n",
    "\n",
    "1. **Aggregation of results** \n",
    "2. **Broadcasts**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common uses of accumulators is to count events that occur during job execution for debugging purposes. For example, say that we are loading a list of all of the call signs for which we want to retrieve logs from a file, but we are also interested in how many lines of the input file were blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'W8PAL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = sc.textFile(\"callsign.txt\")\n",
    "\n",
    "blankLines = sc.accumulator(0)\n",
    "\n",
    "def extractCallSigns(line):\n",
    "    global blankLines\n",
    "    if (line == \"\"):\n",
    "        blankLines += 1\n",
    "    return line.split(\" \")\n",
    "\n",
    "callSigns = file.flatMap(extractCallSigns)\n",
    "\n",
    "callSigns.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank lines: 0\n"
     ]
    }
   ],
   "source": [
    "print \"Blank lines: %d\" % blankLines.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulator's value becomes valid **ONLY** after the an action is run on rdd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use accumulator in spark:\n",
    "\n",
    "1. We create them in the driver by calling the **SparkContext.accumulator**(**initial Value**) method, which produces an accumulator holding an initial value. The return type is an **org.apache.spark.Accumulator[T]** object, where **T** is the type of **initialValue**.\n",
    "2. Worker code in Spark closures can add to the accumulator with its **+=** method.\n",
    "3. The driver program can call the **value** property on the accumulator to access its value.\n",
    "\n",
    "Tasks on worker nodes cannot access the accumulator's **value**(). The value of accumulator is available **only in the driver program**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example1: ham radio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'KK6JKQ', 1),\n",
       " (u'K2AMH', 1),\n",
       " (u'VE2UN', 1),\n",
       " (u'OH2TI', 1),\n",
       " (u'N7ICE', 1),\n",
       " (u'VE2CUA', 1),\n",
       " (u'UA1LO', 1),\n",
       " (u'W8PAL', 1),\n",
       " (u'W6BB', 1),\n",
       " (u'GB1MIR', 1),\n",
       " (u'VE3UOW', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "validSignCount = sc.accumulator(0)\n",
    "invalidSignCount = sc.accumulator(0)\n",
    "\n",
    "def validateSign(sign):\n",
    "    global validSignCount, invalidSignCount\n",
    "    if re.match(r\"\\A\\d?[a-zA-Z]{1,2}\\d{1,4}[a-zA-Z]{1,3}\\Z\", sign):\n",
    "        validSignCount += 1\n",
    "        return True\n",
    "    else:\n",
    "        invalidSignCount += 1\n",
    "        return False\n",
    "\n",
    "validSigns = callSigns.filter(validateSign)\n",
    "contactCount = validSigns.map(lambda sign: (sign, 1)).reduceByKey(lambda (x, y): x + y)\n",
    "\n",
    "contactCount.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validSignCount.value, invalidSignCount.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "\n",
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))\n",
    "\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators and Fault Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an rdd were to be **reevaluated** (failed or evicted from cache):\n",
    "\n",
    "1. If an accumulator was updated in an **action** on this rdd, reevaluation would **not** update the value of accumulator again (**safe**).\n",
    "2. If an accumulator was updated in a **transformation** on this rdd, reevaluation would update the value of accumulator again (**unsafe**). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Accumulators\n",
    "\n",
    "Only works on operations that are **associative** and **commutative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find minimum value using custom accumulator\n",
    "\n",
    "import sys\n",
    "from pyspark import AccumulatorParam\n",
    "\n",
    "class MinAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, initialValue):\n",
    "        return initialValue\n",
    "    def addInPlace(self, v1, v2):\n",
    "        return v1 if v1 < v2 else v2\n",
    "\n",
    "accum = sc.accumulator(sys.maxint, MinAccumulatorParam())\n",
    "\n",
    "rdd = sc.parallelize([3, -1, 4, -1, 5, -9, 2, -6])\n",
    "rdd.foreach(lambda x: accum.add(x))\n",
    "\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's second type of shared variable, broadcast variables, allows the program to efficiently send a **large**, **read-only** value to all the worker nodes for use in one or more Spark operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, -4, -1, 5, 9]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br = sc.broadcast({\"a\": 3, \"b\": -4, \"c\": -1, \"d\": 5, \"e\": 9})\n",
    "rdd = sc.parallelize(list(\"abcde\"))\n",
    "\n",
    "observedSizes = rdd.map(lambda x: br.value[x])\n",
    "\n",
    "observedSizes.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of using broadcast variables is simple:\n",
    "1. Create a **Broadcast[T]** by calling **SparkContext.broadcast** on an object of type **T**. Any type works as long as it is also Serializable.\n",
    "2. Access its value with the **value** property.\n",
    "3. The variable will be sent to each node only once, and should be treated as read-only (updates will not be propagated to other nodes).\n",
    "\n",
    "### Optimizing Broadcasts\n",
    "\n",
    "It is important to choose a data serialization format that is both fast and compact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on a Per-Partition Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with data on a per-partition basis allows us to avoid redoing setup work for each data item. Operations like opening a database connection or creating a random-number generator are examples of setup steps that we wish to avoid doing for each element. Spark has per-partition versions of map and foreach to help reduce the cost of these operations by letting you run code only once for each partition of an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'http://73s.com/qsos/KK6JKQ.json',\n",
       " [{u'address': u'330 N. Mathilda ave #204',\n",
       "   u'age': None,\n",
       "   u'arrl_sect': None,\n",
       "   u'band': u'40m',\n",
       "   u'callsign': u'KK6JLK',\n",
       "   u'city': u'SUNNYVALE',\n",
       "   u'comment': u'',\n",
       "   u'confirmcode': u'ix3kq728g7w2ns6mpe3plb9d',\n",
       "   u'confirmtime': None,\n",
       "   u'contactgrid': u'CM87xj',\n",
       "   u'contactlat': u'37.384733',\n",
       "   u'contactlong': u'-122.032164',\n",
       "   u'contacttime': u'2014-02-08T23:51:00Z',\n",
       "   u'contestid': None,\n",
       "   u'continent': None,\n",
       "   u'country': u'United States',\n",
       "   u'county': u'Santa Clara',\n",
       "   u'cqzone': None,\n",
       "   u'created_at': u'2014-08-13T23:52:06Z',\n",
       "   u'dxcc': u'291',\n",
       "   u'email': u'',\n",
       "   u'event': u'',\n",
       "   u'frequency': u'',\n",
       "   u'fullname': u'MATTHEW McPherrin',\n",
       "   u'id': 57779,\n",
       "   u'image': u'',\n",
       "   u'iota': None,\n",
       "   u'ituzone': None,\n",
       "   u'mode': u'FM',\n",
       "   u'mygrid': u'CM87ss',\n",
       "   u'mylat': u'37.7519528215759',\n",
       "   u'mylong': u'-122.42086887359619',\n",
       "   u'notes': None,\n",
       "   u'operator': None,\n",
       "   u'propmode': None,\n",
       "   u'qslmessage': None,\n",
       "   u'qslreceived': None,\n",
       "   u'qslreceivedate': None,\n",
       "   u'qslsent': None,\n",
       "   u'qslsentdate': None,\n",
       "   u'qslvia': None,\n",
       "   u'qth': None,\n",
       "   u'rstreceived': None,\n",
       "   u'rstsent': None,\n",
       "   u'rxpower': u'',\n",
       "   u'rxsignal': u'',\n",
       "   u'satellitemode': None,\n",
       "   u'satellitename': None,\n",
       "   u'serialreceived': None,\n",
       "   u'serialsent': None,\n",
       "   u'state': u'CA',\n",
       "   u'tenten': None,\n",
       "   u'timeoff': None,\n",
       "   u'timeon': None,\n",
       "   u'txpower': u'',\n",
       "   u'txsignal': u'',\n",
       "   u'updated_at': u'2014-09-04T20:07:41Z',\n",
       "   u'user_id': 3672,\n",
       "   u'veprov': None,\n",
       "   u'wpxprefix': None,\n",
       "   u'zip': u'94085'}])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib3\n",
    "import json\n",
    "\n",
    "def processCallSigns(signs):\n",
    "    \"\"\"Lookup call signs using a connection pool\"\"\"\n",
    "    # Create a connection pool\n",
    "    http = urllib3.PoolManager()\n",
    "    # the URL associated with each call sign record\n",
    "    urls = map(lambda x: \"http://73s.com/qsos/%s.json\" % x, signs)\n",
    "    # create the requests (non-blocking)\n",
    "    requests = map(lambda x: (x, http.request('GET', x)), urls)\n",
    "    # fetch the results\n",
    "    result = map(lambda x: (x[0], json.loads(x[1].data)), requests)\n",
    "    # remove any empty results and return\n",
    "    return filter(lambda x: x[1] is not None, result)\n",
    "\n",
    "def fetchCallSigns(input):\n",
    "    \"\"\"Fetch call signs\"\"\"\n",
    "    return input.mapPartitions(lambda callSigns : processCallSigns(callSigns))\n",
    "\n",
    "contactsContactList = fetchCallSigns(validSigns)\n",
    "\n",
    "contactsContactList.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to avoiding setup work, we can sometimes use **mapPartitions()** to avoid object creation overhead. Sometimes we need to make an object for aggregating the result that is of a different type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we computed the average of a list of numbers, one of the ways we did this was by converting our RDD of numbers to an RDD of tuples so we could track the number of elements processed in our reduce step. \n",
    "\n",
    "**Average without mapPartitions() in Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combineCtrs(c1, c2):\n",
    "    return (c1[0] + c2[0], c1[1] + c2[1])\n",
    "\n",
    "nums = sc.parallelize(range(10), 2)\n",
    "nums.map(lambda num: (num, 1)).reduce(combineCtrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing this for each element, we can instead create the tuple once per partition.\n",
    "\n",
    "**Average with mapPartition() in Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def partitionCtr(nums):\n",
    "    sumCount = [0, 0]\n",
    "    for num in nums:\n",
    "        sumCount[0] += num\n",
    "        sumCount[1] += 1\n",
    "\n",
    "    return [sumCount]\n",
    "\n",
    "sumCount = nums.mapPartitions(partitionCtr).reduce(combineCtrs)\n",
    "\n",
    "sumCount[0] / float(sumCount[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mapPartitions(f, preservesPartitioning=False)**\n",
    "\n",
    "Return a new RDD by applying a function to each partition of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 40]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2)\n",
    "\n",
    "def f(iterator): yield sum(iterator)\n",
    "    \n",
    "rdd.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides several descriptive statistics operations on RDDs containing numeric data. \n",
    "\n",
    "Spark's numeric operations are implemented with a streaming algorithm that allows for building up our model one element at a time. The descriptive statistics are all computed in a single pass over the data and returned as a StatsCounter object by\n",
    "calling **stats()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 4.541629\n",
      "std: 2.932548\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "distances = sc.parallelize(np.abs(np.random.rand(100) * 10)).persist()\n",
    "\n",
    "# Compute statistics uing actions on RDD\n",
    "print \"mean: %f\" % distances.mean()\n",
    "print \"std: %f\" % distances.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.6849729354803262,\n",
       " 1.0060285724857188,\n",
       " 5.6319080113919728,\n",
       " 6.9631208140385663,\n",
       " 4.3241829286522639]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OR use the StatsCounter() object from RDD\n",
    "stats = distances.stats()\n",
    "stddev = stats.stdev()\n",
    "mean = stats.mean()\n",
    "reasonableDistances = distances.filter(lambda x: math.fabs(x - mean) < 3 * stddev)\n",
    "\n",
    "reasonableDistances.take(5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
