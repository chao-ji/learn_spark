{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"pyspark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pair RDD using the first word as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = lines.map(lambda x: (x.split(\" \")[0], x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, for the functions on keyed data to work we need to return RDD composed of tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on Pair RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair RDDs are allowed to use all the transformations available to standard RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out lines longer than 20 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pairs.filter(lambda keyValue: len(keyValue[1]) <= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example on filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = sc.parallelize([(\"holden\", \"likes coffee\"), (\"panda\", \"likes long strings and coffee\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('holden', 'likes coffee')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.filter(lambda keyValue: len(keyValue[1]) <= 20).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to work with the value part of a key-value tuple, use **mapValues()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (3, 5), (3, 7)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduceByKey()** runs several parallel reduce operations, one for each key in the dataset, where each operation combines values that have the same key. It returns a new RDD (i.e. transformation) consisting of each key and the reduced value for that key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"panda\", 0), (\"pink\", 3), (\"pirate\", 3), (\"panda\", 1), (\"pink\", 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', 0), ('pink', 3), ('pirate', 3), ('panda', 1), ('pink', 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', (0, 1)),\n",
       " ('pink', (3, 1)),\n",
       " ('pirate', (3, 1)),\n",
       " ('panda', (1, 1)),\n",
       " ('pink', (4, 1))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(lambda x: (x, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pink', (7, 2)), ('panda', (1, 2)), ('pirate', (3, 1))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = rdd.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways: 1) using **map()** and **reduceByKey()**; 2) using **countByValue()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = words.countByValue()\n",
    "\n",
    "len(mydict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**combineByKey()** is the most general of the per-key aggregation functions.\n",
    "\n",
    "**combineByKey()** allows the user to return values that are not the same type as our input data.\n",
    "\n",
    "\n",
    "1. As **combineByKey()** goes through the elements in a partition, each element either has a key it hasn't seen before or has the same key as a previous element.\n",
    "\n",
    "2. If it's a new element, **combineByKey()** uses a function we provide, called create **Combiner()**, to create the initial value for the accumulator on that key. This happens the first time a key is found **in each partition**, rather than only the first time the key is found in the RDD.\n",
    "\n",
    "3. If it is a value we have seen before while processing that partition, it will instead use the provided function, **mergeValue()**, with the current value for the accumulator for that key and the new value.\n",
    "\n",
    "4. Since each partition is processed independently, we can have multiple accumulators for the same key. When we are merging the results from each partition, if two or more partitions have an accumulator for the same key we merge the accumulators using the user-supplied **mergeCombiners()** function.\n",
    "\n",
    "**combineByKey(createCombiner, mergeValue, mergeCombiners)**\n",
    "\n",
    "Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a “combined type” C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([(\"panda\", 0), (\"pink\", 3), (\"pirate\", 3), (\"panda\", 1), (\"pink\", 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('panda', 0), ('pink', 3), ('pirate', 3), ('panda', 1), ('pink', 4)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumCount = nums.combineByKey((lambda x: (x, 1)),\n",
    "                             # createCombiner()\n",
    "                             (lambda x, y: (x[0] + y, x[1] + 1)),\n",
    "                             # mergeValue() Map-side aggregation\n",
    "                             (lambda x, y: (x[0] + y[0], x[1] + y[1])))\n",
    "                             # mergeCombiners() Aggregation across partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pink', (7, 2)), ('panda', (1, 2)), ('pirate', (3, 1))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumCount.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', '191'), ('b', '1')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 91)])\n",
    "def add(a, b): return a + str(b)\n",
    "sorted(rdd.combineByKey(str,\n",
    "                        # createCombiner, which turns a V into a C (e.g., creates a one-element list)\n",
    "                        add,\n",
    "                        # mergeValue, to merge a V into a C (e.g., adds it to the end of a list)\n",
    "                        add\n",
    "                        # mergeCombiners, to combine two C’s into a single one.\n",
    "                       ).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3**:\n",
    "\n",
    "Use **combineByKey()** to implement **reduceByKey()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 9)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 2), (\"b\", 9), (\"a\", 1)])\n",
    "rdd.combineByKey(lambda x: x,\n",
    "                 lambda x, y: x + y,\n",
    "                 lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 2), (\"b\", 9), (\"a\", 1)])\n",
    "rdd.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the level of parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every RDD has a fixed number of *partitions* that determine the degree of parallelism to use when executing operations on the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark always try to infer a sensible default number of partitions based on the size of your cluster, but we can aske Spark to use a specific value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [(\"a\", 3), (\"b\", 4), (\"a\", 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[50] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(data).reduceByKey(lambda x, y: x + y)\n",
    "# Default parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[56] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10)\n",
    "# Custom parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**repartition()** shuffles the data across the network to create a new set of partitions (VERY EXPENSIVE).\n",
    "\n",
    "**coalesce()** is a special case of **repartition()**: it allows you to decrease the number of RDD partitions (i.e. merging existing partitions)\n",
    "\n",
    "Use **rdd.getNumPartitions()** to get the number of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**groupByKey()** on pair RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x7f203c21a690>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x7f203c21ad10>)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 3), (\"b\", 4), (\"a\", 1)])\n",
    "rdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**groupBy()** on unpaired RDD\n",
    "\n",
    "It takes a function that it applies to every element in the source RDD and uses the result to determine the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 8]), (1, [1, 1, 3, 5])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result = rdd.groupBy(lambda x: x % 2).collect()\n",
    "\n",
    "[(x, list(y)) for (x, y) in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **groupByKey()** and **mapValues()** to implement **reduceByKey()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\", 3), (\"b\", 4), (\"a\", 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4), ('b', 4)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey().mapValues(lambda x: reduce(lambda a, b: a + b, x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4), ('b', 4)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cogroup()** over two RDDs sharing the same key type, K, with the respective value types V and W gives us back RDD[(K, (Iterable[V], Iterable[W]))]. If one of the RDDs doesn't have elements for a given key that is present in the other RDD, the corresponding Iterable is simply empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"a\", 2)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"b\", 6), (\"c\", 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = x.cogroup(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f203c23b710>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f203c22c550>)),\n",
       " ('c',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f203c22c150>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f203c22c610>)),\n",
       " ('b',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f203c22ca90>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f203c22ca50>))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ([1, 2], [2])), ('c', ([], [3])), ('b', ([4], [6]))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(item[0], tuple([list(i) for i in item[1]])) for item in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storeAddress = sc.parallelize([(\"Ritual\", \"1026 Valencia St\"),\n",
    "                               (\"Philz\", \"748 Van Ness Ave\"),\n",
    "                               (\"Philz\", \"3101 24th St\"),\n",
    "                               (\"Starbucks\", \"Seattle\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storeRating = sc.parallelize([(\"Ritual\", 4.9), (\"Philz\", 4.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Philz', ('748 Van Ness Ave', 4.8)),\n",
       " ('Philz', ('3101 24th St', 4.8)),\n",
       " ('Ritual', ('1026 Valencia St', 4.9))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storeAddress.join(storeRating).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Philz', ('748 Van Ness Ave', 4.8)),\n",
       " ('Philz', ('3101 24th St', 4.8)),\n",
       " ('Ritual', ('1026 Valencia St', 4.9)),\n",
       " ('Starbucks', ('Seattle', None))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storeAddress.leftOuterJoin(storeRating).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Philz', ('748 Van Ness Ave', 4.8)),\n",
       " ('Philz', ('3101 24th St', 4.8)),\n",
       " ('Ritual', ('1026 Valencia St', 4.9))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storeAddress.rightOuterJoin(storeRating).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"3\", 312), (\"1\", 394), (\"4\", 903),\n",
    "                      (\"1\", 394), (\"5\", 105), (\"9\", 967),\n",
    "                      (\"2\", 234), (\"6\", 831)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 394),\n",
       " ('1', 394),\n",
       " ('2', 234),\n",
       " ('3', 312),\n",
       " ('4', 903),\n",
       " ('5', 105),\n",
       " ('6', 831),\n",
       " ('9', 967)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortByKey(ascending=True,\n",
    "              numPartitions=None,\n",
    "              keyfunc=lambda x: str(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions Available on Pair RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Partitioning (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Use **partiionBy()** (transformation) to partition (i.e. hash-partition) the RDD prior to join operations.\n",
    "2. Use **persist()** to take advantage of the partition in the privous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "1. **sortByKey()** results in range-partitioned RDDs|\n",
    "2. **groupByKey()** results in hash-partitioned RDDs|\n",
    "3. **map()** causes new RDD to forget the parent's partitioning info|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations That Benefit from Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of Spark's operations involve shuffling data by key across the network. All of these will benefit from partitioning. As of Spark 1.0, the operations that benefit from partitioning are **cogroup()**, **groupWith()**, **join()**, **leftOuterJoin()**, **rightOuterJoin()**, **groupByKey()**, **reduceByKey()**, **combineByKey()**, and **lookup()**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For operations that act on a single RDD, such as **reduceByKey()**, running on a pre-partitioned RDD will cause all the values for each key to be computed locally on a single machine, requiring only the final, locally reduced value to be sent from each worker node back to the master. \n",
    "\n",
    "2. For binary operations, such as **cogroup()** and **join()** , pre-partitioning will cause at least one of the RDDs (the one with the known partitioner) to not be shuffled. \n",
    "\n",
    "3. If both RDDs have the same partitioner, and if they are cached on the same machines (e.g., one was created using **mapValues()** on the other, which preserves keys and partitioning) or if one of them has not yet been computed, then no shuffling across the network will occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations That Affect Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spark knows internally how each of its operations affects partitioning, and automatically sets the partitioner on RDDs created by operations that partition the data. For example, suppose you called **join()** to join two RDDs; because the elements with the same key have been hashed to the same machine, Spark knows that the result is hash-partitioned, and operations like **reduceByKey()** on the join result are going to be significantly faster.\n",
    "\n",
    "2. The flipside, however, is that for transformations that cannot be guaranteed to produce a known partitioning, the output RDD will not have a partitioner set. For example, if you call **map()** on a hash-partitioned RDD of key/value pairs, the function passed to **map()** can in theory change the key of each element, so the result will not have a partitioner. Spark does not analyze your functions to check whether they retain the key. Instead, it provides two other operations, **mapValues()** and **flatMapValues()**, which guarantee that each tuple's key remains the same.\n",
    "\n",
    "3. All that said, here are all the operations that result in a partitioner being set on the output RDD: **cogroup()**, **groupWith()**, **join()**, **leftOuterJoin()**, **rightOuterJoin()**, **groupByKey()**, **reduceByKey()**, **combineByKey()**, **partitionBy()**, **sort()**, **mapValues()** (if the parent RDD has a partitioner), **flatMapValues()** (if parent has a partitioner), and **filter()** (if parent has a partitioner). All other operations will produce a result with no partitioner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', [u'b', u'd']),\n",
       " (u'c', [u'd']),\n",
       " (u'e', [u'a', u'b', u'c']),\n",
       " (u'b', [u'c', u'e']),\n",
       " (u'd', [u'e'])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = sc.textFile(\"links\").map(lambda x: x.split(\"\\t\")).map(lambda x: (x[0], x[1:])).partitionBy(2).persist()\n",
    "links.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'a', 0.2), (u'b', 0.2), (u'c', 0.2), (u'e', 0.2), (u'd', 0.2)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = sc.textFile(\"ranks\").map(lambda x: x.split(\"\\t\")).map(lambda x: (x[0], float(x[1])))\n",
    "ranks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'e', 0.33472222222222225),\n",
       " (u'b', 0.1518518518518519),\n",
       " (u'a', 0.10740740740740744),\n",
       " (u'd', 0.22222222222222227),\n",
       " (u'c', 0.18379629629629635)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX = 5\n",
    "for c in range(MAX):\n",
    "    contributions = links.join(ranks).flatMap(lambda x: [(i, x[1][1]/len(x[1][0])) for i in x[1][0]])\n",
    "    ranks = contributions.reduceByKey(lambda x, y: x + y)\n",
    "    \n",
    "ranks.collect()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Partitioners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[187] at mapPartitions at PythonRDD.scala:422"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urlparse\n",
    "\n",
    "def hash_domain(url):\n",
    "    return hash(urlparse.urlparse(url).netloc)\n",
    "\n",
    "rdd = sc.parallelize([\"http://www.cnn.com/world\",\n",
    "                      \"http://www.cnn.com/us\",\n",
    "                      \"https://www.google.com/preferences\"])\n",
    "rdd.partitionBy(2, hash_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on one pair RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 **reduceByKey(func)**: Combine values with the same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 **groupByKey()**: Group values with the same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x7f203c26f7d0>),\n",
       " (3, <pyspark.resultiterable.ResultIterable at 0x7f203c1d75d0>)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 **combineByKey(createCombiner, mergeValue, mergeCombiner, partitioner)**: Combine values with the same key using a different result type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 1)), (3, (10, 2))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.combineByKey((lambda x: (x, 1)),\n",
    "                 (lambda x, y: (x[0] + y, x[1] + 1)),\n",
    "                 (lambda x, y: (x[0] + y[0], x[1] + y[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 **mapValues(func)**: Apply a function to each value of a pair RDD without changing the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (3, 5), (3, 7)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 **flatMapValues(func)**: Apply a function that returns an iterator to each value of a pair RDD, and for each element returned produce a key/value entry with the old key. Often used to tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 3), (1, 4), (1, 5), (3, 4), (3, 5)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMapValues(lambda x: range(x, 6)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 **key()**: Return an RDD of just the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 3]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 **value()**: Return ad RDD of just the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 **sortByKey()**: Return an RDD sorted by the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (3, 6)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 **combineByKey(createCombiner, mergeValue, mergeCombiners)**\n",
    "\n",
    "Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a “combined type” C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', '191'), ('b', '1')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 91)])\n",
    "def add(a, b): return a + str(b)\n",
    "sorted(rdd.combineByKey(str,\n",
    "                        # createCombiner, which turns a V into a C (e.g., creates a one-element list)\n",
    "                        add,\n",
    "                        # mergeValue, to merge a V into a C (e.g., adds it to the end of a list)\n",
    "                        add\n",
    "                        # mergeCombiners, to combine two C’s into a single one.\n",
    "                       ).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation on two pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other = sc.parallelize([(3, 9)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 **subtractByKey()**: Remove elements with a key present in the other RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.subtractByKey(other).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 **join()**: Perform an inner join between two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, (4, 9)), (3, (6, 9))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.join(other).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 **rightOuterJoin()**: Perform a right-outer join: rdd(left), other(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, (4, 9)), (3, (6, 9))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.rightOuterJoin(other).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 **leftOuterJoin()**: Perform a left-outer join: rdd(left), other(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, None)), (3, (4, 9)), (3, (6, 9))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.leftOuterJoin(other).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 **cogroup()**: Group data from both RDDs sharing the same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f203c1d7bd0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f203c1d7890>)),\n",
       " (3,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f203c1e6f90>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f203c1e6fd0>))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cogroup(other).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Actions on pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 **countByKey()**: Count the number of elements for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 1, 3: 2})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 **collectAsMap()**: Collect the result as a map to provide easy lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 3: 6}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 **lookup()**: Return all values associated with the provided key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.lookup(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
